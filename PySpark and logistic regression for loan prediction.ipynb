{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Aim:\n",
    "The project aims to make use of Python and Spark to extract insights from the data.\n",
    "\n",
    "Secondly, to learn how to use ML Pipeline which provides a uniform set of high-level APIs on top of DataFrames.\n",
    "\n",
    "And in the end, to predict whether the loan applicant can replay the loan or not using logistic regression.\n",
    "\n",
    "### Attributes in the dataset:\n",
    "Loan id, Gender, Married, Dependents, Education, Self Employed, Applicant income, Coapplicant income, Loan Amount,Credit History, Property_Area, Loan_Status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/meghdad/spark-2.4.5-bin-hadoop2.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"loan_prediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614 13\n",
      "root\n",
      " |-- Loan_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Married: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- Education: string (nullable = true)\n",
      " |-- Self_Employed: string (nullable = true)\n",
      " |-- ApplicantIncome: integer (nullable = true)\n",
      " |-- CoapplicantIncome: double (nullable = true)\n",
      " |-- LoanAmount: integer (nullable = true)\n",
      " |-- Loan_Amount_Term: integer (nullable = true)\n",
      " |-- Credit_History: integer (nullable = true)\n",
      " |-- Property_Area: string (nullable = true)\n",
      " |-- Loan_Status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"loan_prediction.csv\",header=True,inferSchema=True)\n",
    "\n",
    "## number of rows and columns\n",
    "print(df.count(),len(df.columns))\n",
    "\n",
    "## variable types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|target|count|\n",
      "+------+-----+\n",
      "|     1|  422|\n",
      "|     0|  192|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "## create integer target variable \n",
    "df=df.withColumn(\"target\",when(df[\"Loan_Status\"]==\"Y\",1).otherwise(0))\n",
    "df.groupBy(\"target\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male\n",
      "No\n",
      "0\n",
      "Graduate\n",
      "No\n",
      "5849\n",
      "0.0\n",
      "None\n",
      "360\n",
      "1\n",
      "Urban\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "## delete loan_ID and Loan_Status \n",
    "df=df.drop(\"Loan_ID\",\"Loan_Status\")\n",
    "\n",
    "for item in df.head(1)[0]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+---------+-------------+---------------+-----------------+----------+----------------+--------------+-------------+------+\n",
      "|Gender|Married|Dependents|Education|Self_Employed|ApplicantIncome|CoapplicantIncome|LoanAmount|Loan_Amount_Term|Credit_History|Property_Area|target|\n",
      "+------+-------+----------+---------+-------------+---------------+-----------------+----------+----------------+--------------+-------------+------+\n",
      "|    13|      3|        15|        0|           32|              0|                0|        22|              14|            50|            0|     0|\n",
      "+------+-------+----------+---------+-------------+---------------+-----------------+----------+----------------+--------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## missing values\n",
    "from pyspark.sql.functions import col,sum\n",
    "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "## seperating string and numeric columns\n",
    "string_cols=[item[0] for item in df.dtypes if item[1].startswith(\"string\") ]\n",
    "numeric_cols=[item[0] for item in df.dtypes if item[1].startswith(\"int\" or \"double\")]\n",
    "\n",
    "## exclude target variable \n",
    "numeric_cols=numeric_cols[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5403.459283387622\n",
      "146.41216216216216\n",
      "342.0\n",
      "0.8421985815602837\n"
     ]
    }
   ],
   "source": [
    "## filling missing values with numeric column's mean\n",
    "for item in numeric_cols:\n",
    "    mean_col=df.groupBy().mean(item).collect()[0][0]\n",
    "    print(mean_col)\n",
    "    df=df.na.fill(mean_col,subset=[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male\n",
      "Yes\n",
      "0\n",
      "Graduate\n",
      "No\n",
      "Semiurban\n"
     ]
    }
   ],
   "source": [
    "## filling missing values with string column's mode\n",
    "for item in string_cols:\n",
    "    df_mode=df.groupBy(item).count()\n",
    "    mode_col=df_mode.orderBy(df_mode[\"Count\"].desc()).collect()[0][0]\n",
    "    print(mode_col)\n",
    "    df=df.na.fill(mode_col,subset=[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up DataFrame for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (StringIndexer,\n",
    "                                OneHotEncoder,\n",
    "                                VectorAssembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ApplicantIncome\n",
      "LoanAmount\n",
      "Loan_Amount_Term\n",
      "Credit_History\n",
      "Gender_Index_Vec\n",
      "Married_Index_Vec\n",
      "Dependents_Index_Vec\n",
      "Education_Index_Vec\n",
      "Self_Employed_Index_Vec\n",
      "Property_Area_Index_Vec\n"
     ]
    }
   ],
   "source": [
    "## the index of string values multiple columns \n",
    "indexers=[StringIndexer(inputCol=col,\n",
    "                        outputCol=\"{0}_Index\".format(col)) \n",
    "          for col in string_cols]\n",
    "\n",
    "## the encode of indexed values multiple columns\n",
    "encoders=[OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
    "                       outputCol=\"{0}_Vec\".format(indexer.getOutputCol())) \n",
    "         for indexer in indexers]\n",
    "\n",
    "## combine numeric and encoded string columns\n",
    "assb_cols=numeric_cols+[encoder.getOutputCol() for encoder in encoders]\n",
    "\n",
    "## input columns for vector assembler\n",
    "for item in assb_cols:\n",
    "    print(item)\n",
    "\n",
    "assembler=VectorAssembler(inputCols=assb_cols,outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg=LogisticRegression(featuresCol=\"features\",labelCol=\"target\")\n",
    "\n",
    "pipeline=Pipeline(stages=indexers+encoders+[assembler,log_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Gender: string, Married: string, Dependents: string, Education: string, Self_Employed: string, ApplicantIncome: int, CoapplicantIncome: double, LoanAmount: int, Loan_Amount_Term: int, Credit_History: int, Property_Area: string, target: int, Gender_Index: double, Married_Index: double, Dependents_Index: double, Education_Index: double, Self_Employed_Index: double, Property_Area_Index: double, Gender_Index_Vec: vector, Married_Index_Vec: vector, Dependents_Index_Vec: vector, Education_Index_Vec: vector, Self_Employed_Index_Vec: vector, Property_Area_Index_Vec: vector, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model=pipeline.fit(train_data)\n",
    "predictions=fit_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction|target|\n",
      "+----------+------+\n",
      "|       1.0|     1|\n",
      "|       0.0|     0|\n",
      "|       0.0|     1|\n",
      "|       1.0|     1|\n",
      "|       1.0|     1|\n",
      "+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\",\"target\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 24.  18.]\n",
      " [  8. 113.]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "## need to cast to float type and order by prediction else won't work\n",
    "preds_and_labels=predictions.select(\"prediction\",\"target\").withColumn(\"label\",col(\"target\").cast(FloatType())).orderBy(\"prediction\")\n",
    "## select only prediction and label columns\n",
    "preds_and_labels=preds_and_labels.select(\"prediction\",\"label\")\n",
    "metrics=MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC is : 0.7526564344746163\n"
     ]
    }
   ],
   "source": [
    "my_eval=BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"target\")\n",
    "AUC=my_eval.evaluate(predictions)\n",
    "print(\"AUC is :\", AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on new data\n",
    "In case of prediction on new data the pipeline should not include classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=Pipeline(stages=indexers+encoders+[assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[5849.0,146.0,360...|\n",
      "|[4583.0,128.0,360...|\n",
      "|[3000.0,66.0,360....|\n",
      "|[2583.0,120.0,360...|\n",
      "|[6000.0,141.0,360...|\n",
      "|[5417.0,267.0,360...|\n",
      "|[2333.0,95.0,360....|\n",
      "|[3036.0,158.0,360...|\n",
      "|[4006.0,168.0,360...|\n",
      "|[12841.0,349.0,36...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data=pipeline.fit(df).transform(df)\n",
    "final_data.select(\"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg=LogisticRegression(featuresCol=\"features\",labelCol=\"target\")\n",
    "final_model=log_reg.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows:  367 , No. of columns:  12\n"
     ]
    }
   ],
   "source": [
    "unlabeled=spark.read.csv(\"unlabeled_loan.csv\",header=True,inferSchema=True)\n",
    "\n",
    "print(\"No. of rows: \",unlabeled.count(),\", No. of columns: \",len(unlabeled.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Loan_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Married: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- Education: string (nullable = true)\n",
      " |-- Self_Employed: string (nullable = true)\n",
      " |-- ApplicantIncome: integer (nullable = true)\n",
      " |-- CoapplicantIncome: integer (nullable = true)\n",
      " |-- LoanAmount: integer (nullable = true)\n",
      " |-- Loan_Amount_Term: integer (nullable = true)\n",
      " |-- Credit_History: integer (nullable = true)\n",
      " |-- Property_Area: string (nullable = true)\n",
      " |-- Gender_Index: double (nullable = false)\n",
      " |-- Married_Index: double (nullable = false)\n",
      " |-- Dependents_Index: double (nullable = false)\n",
      " |-- Education_Index: double (nullable = false)\n",
      " |-- Self_Employed_Index: double (nullable = false)\n",
      " |-- Property_Area_Index: double (nullable = false)\n",
      " |-- Gender_Index_Vec: vector (nullable = true)\n",
      " |-- Married_Index_Vec: vector (nullable = true)\n",
      " |-- Dependents_Index_Vec: vector (nullable = true)\n",
      " |-- Education_Index_Vec: vector (nullable = true)\n",
      " |-- Self_Employed_Index_Vec: vector (nullable = true)\n",
      " |-- Property_Area_Index_Vec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=pipeline.fit(unlabeled).transform(unlabeled)\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_predictions=final_model.transform(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
